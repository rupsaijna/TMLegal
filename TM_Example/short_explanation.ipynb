{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "#!pip install pyTsetlinMachineCUDA spacy pycuda unidecode\n",
    "#!python3 -m spacy download en_core_web_sm\n",
    "from PyTsetlinMachineCUDA.tm import MultiClassTsetlinMachine\n",
    "from time import time\n",
    "import random\n",
    "\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os, sys, inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir) \n",
    "from data_handling.data_tokenization import get_dataset_for_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating vocabulary: 100%|██████████| 40/40 [01:07<00:00,  1.68s/it]\n",
      "Creating Token to Index Mapping:   4%|▍         | 15443/389616 [00:00<00:00, 2053080.50it/s]\n",
      "Creating subparts and labels:   5%|▌         | 2/40 [00:00<00:02, 15.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing file with same configuration, creating new file....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating subparts and labels: 100%|██████████| 40/40 [00:02<00:00, 14.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: 0.4\n",
      "Pos train: 18\n",
      "Neg train: 22\n",
      "Batch size: 13\n"
     ]
    }
   ],
   "source": [
    "def get_train_test_dataset_for_category(category, num_examples=255, subpart_size=512, subpart_overlap=26):\n",
    "    data_source = \"../cuad_json/CUADv1.json\"\n",
    "    num_examples = num_examples # 510 is max\n",
    "    subpart_size = subpart_size\n",
    "    subpart_overlap = subpart_overlap\n",
    "    data_destination = f\"datasets/binary_dataset_{subpart_size}_{subpart_overlap}_{num_examples}.json\"\n",
    "    vocab_destination = f\"vocabs/vocab_{num_examples}.json\"\n",
    "    category = category\n",
    "\n",
    "    # Save everything the Spacy tokenizer gives us\n",
    "    tokenize = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    train_dataset, test_dataset, tokenizer, vocab_to_idx = get_dataset_for_category(category, data_source, data_destination, vocab_destination, num_examples, subpart_size, subpart_overlap, tokenize)\n",
    "    \n",
    "    return train_dataset, test_dataset, tokenizer, vocab_to_idx\n",
    "\n",
    "category = \"termination_for_convenience\" # One of the categories which FastText had the highest soft doc accuracies with\n",
    "num_docs = 40\n",
    "subpart_size = 512 # 512 tokens per chunk\n",
    "subpart_overlap = 26 # 26 tokens overlaps between each chunk\n",
    "\n",
    "train_dataset, test_dataset, tokenizer, vocab_to_idx = get_train_test_dataset_for_category(category, num_docs, subpart_size, subpart_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio: The ratio between negative and positive docs. This ratio is what is used for weighing the nr of positive and negative chunks in the training batches\n",
    "Just ignore the pos train and neg train\n",
    "Batch size: Nr of chunks per batch. There are *num_docs* batches generated with *batch_size* nr of chunks per batch. Resulting in a total number of chunks: *num_docs* * *batch_size*, which in this case is: 40 * 13 = 520 training chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of batches in the training dataset: 40\n",
      "Batchlen of batches in the training dataset: 13\n",
      "Nr of tokens in a chunk: 512\n",
      "Nr of labels per chunk: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Nr of batches in the training dataset:\", len(train_dataset))\n",
    "print(\"Batchlen of batches in the training dataset:\", len(train_dataset[0]))\n",
    "print(\"Nr of tokens in a chunk:\", len(train_dataset[0][0][\"subpart\"]))\n",
    "print(\"Nr of labels per chunk:\", train_dataset[0][3][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if shuffling of the data is important for the Tsetlin Machine, but here it is atleast\n",
    "for i in range(len(train_dataset)):\n",
    "    random.shuffle(train_dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure train_dataset\n",
    "\n",
    "So the structure of the train_dataset is:\n",
    "\n",
    "`len(train_dataset) = nr of batches`\n",
    "\n",
    "`len(train_dataset[0]) = nr of chunks in batch (batch_size)`\n",
    "\n",
    "`train_dataset[0][0] = dict(\"subpart\": list(tokens) , \"label\": int)`\n",
    "\n",
    "\n",
    "For the test_dataset it is a little different because it is document based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Batch size' in doc: 1 is: 7\n",
      "'Batch size' in doc: 2 is: 9\n",
      "'Batch size' in doc: 3 is: 28\n",
      "'Batch size' in doc: 4 is: 9\n",
      "'Batch size' in doc: 5 is: 32\n",
      "'Batch size' in doc: 6 is: 62\n",
      "'Batch size' in doc: 7 is: 51\n",
      "'Batch size' in doc: 8 is: 20\n",
      "'Batch size' in doc: 9 is: 13\n",
      "'Batch size' in doc: 10 is: 8\n",
      "'Batch size' in doc: 11 is: 23\n",
      "'Batch size' in doc: 12 is: 36\n",
      "'Batch size' in doc: 13 is: 12\n"
     ]
    }
   ],
   "source": [
    "for idx, i in enumerate(test_dataset):\n",
    "    print(\"'Batch size' in doc:\", idx + 1, \"is:\", len(i[\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, the structure is somewhat different. Instead of containing a list of dict(\"subpart\": list(token), \"label\": int) it is rather a dict of lists.\n",
    "\n",
    "test_dataset[0] = dict(\"labels\": list(label), \"subparts\": list(list(token)))\n",
    "\n",
    "Please ask if something is unclear!\n",
    "\n",
    "Below I've just pasted in the method I'm using for getting the data ready for the Tsetlin Machine in regards of bitwise transformation and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {value: key for key, value in vocab_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n",
      "240\n",
      "280\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "for document in train_dataset:\n",
    "    for chunk in document:\n",
    "        train_data.append((chunk[\"label\"], chunk[\"subpart\"]))\n",
    "\n",
    "print(len(train_data))\n",
    "pos = 0\n",
    "neg = 0\n",
    "for i in train_data:\n",
    "    if i[0] == 1:\n",
    "        pos += 1\n",
    "    else:\n",
    "        neg += 1\n",
    "print(pos)\n",
    "print(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51565\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {}\n",
    "\n",
    "MAX_NGRAM = 2\n",
    "\n",
    "for i in train_data:\n",
    "    terms = []\n",
    "    for word_id in i[1]:\n",
    "        terms.append(id_to_word[word_id])\n",
    "\n",
    "    for N in range(1, MAX_NGRAM + 1):\n",
    "        grams = [terms[j:j + N] for j in range(len(terms) - N + 1)]\n",
    "        for gram in grams:\n",
    "            phrase = \" \".join(gram)\n",
    "\n",
    "            if phrase in vocabulary:\n",
    "                vocabulary[phrase] += 1\n",
    "            else:\n",
    "                vocabulary[phrase] = 1\n",
    "                    \n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51565\n"
     ]
    }
   ],
   "source": [
    "phrase_bit_nr = {}\n",
    "bit_nr_phrase = {}\n",
    "bit_nr = 0\n",
    "for phrase in vocabulary.keys():\n",
    "\n",
    "    phrase_bit_nr[phrase] = bit_nr\n",
    "    bit_nr_phrase[bit_nr] = phrase\n",
    "    bit_nr += 1\n",
    "print(len(phrase_bit_nr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating bit representation training data: 100%|██████████| 520/520 [00:01<00:00, 300.19it/s]\n"
     ]
    }
   ],
   "source": [
    "#######################################################################################\n",
    "################################ TRAINING #############################################\n",
    "#######################################################################################\n",
    "\n",
    "X_train = np.zeros((len(train_data), len(phrase_bit_nr)), dtype=np.uint32)\n",
    "Y_train = np.zeros(len(train_data), dtype=np.uint32)\n",
    "\n",
    "for i, item in tqdm(enumerate(train_data), total=len(train_data), desc=\"Creating bit representation training data\"):\n",
    "    terms = []\n",
    "    for word_id in item[1]:\n",
    "        terms.append(id_to_word[word_id])\n",
    "    \n",
    "    for N in range(1, MAX_NGRAM + 1):\n",
    "        grams = [terms[j:j + N] for j in range(len(terms) - N + 1)]\n",
    "        for gram in grams:\n",
    "            phrase = \" \".join(gram)\n",
    "            if phrase in phrase_bit_nr:\n",
    "                X_train[i, phrase_bit_nr[phrase]] += 1\n",
    "\n",
    "    Y_train[i] = item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating bit representation test data:  15%|█▌        | 2/13 [00:00<00:01, 10.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bit representations for the test documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating bit representation test data: 100%|██████████| 13/13 [00:18<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating bit representations for the test documents..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################################################################\n",
    "################################ TESTING ##############################################\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "print(\"Creating bit representations for the test documents...\")\n",
    "\n",
    "test_chunks = []\n",
    "test_labels = []\n",
    "\n",
    "for _, document in tqdm(enumerate(test_dataset), total=len(test_dataset), desc=\"Creating bit representation test data\"):\n",
    "    terms = []\n",
    "    X_test = np.zeros((len(document[\"labels\"]), len(phrase_bit_nr)), dtype=np.uint32)\n",
    "    for i in range(len(document[\"labels\"])):\n",
    "        for word_id in document[\"subparts\"][i]:\n",
    "            terms.append(id_to_word[word_id])\n",
    "            \n",
    "        for N in range(1, MAX_NGRAM + 1):\n",
    "            grams = [terms[j:j + N] for j in range(len(terms) - N + 1)]\n",
    "            for gram in grams:\n",
    "                phrase = \" \".join(gram)\n",
    "                if phrase in phrase_bit_nr:\n",
    "                    X_test[i, phrase_bit_nr[phrase]] += 1\n",
    "\n",
    "    Y_test = np.asarray(document[\"labels\"])\n",
    "    test_chunks.append(X_test)\n",
    "    test_labels.append(Y_test)\n",
    "        \n",
    "print(\"Finished creating bit representations for the test documents..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting features...\n"
     ]
    }
   ],
   "source": [
    "print(\"Selecting features...\")\n",
    "\n",
    "FEATURES = 20_000\n",
    "\n",
    "SKB = SelectKBest(chi2, k=FEATURES)\n",
    "SKB.fit(X_train, Y_train)\n",
    "\n",
    "selected_features = SKB.get_support(indices=True)\n",
    "FEATURES = len(selected_features)\n",
    "X_train = SKB.transform(X_train)\n",
    "\n",
    "for doc in test_chunks:\n",
    "    doc = SKB.transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
